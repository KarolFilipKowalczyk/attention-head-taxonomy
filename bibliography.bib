%=============================================================================
% BIBLIOGRAPHY.BIB - References for Attention Head Naming Convention
%=============================================================================

% Core transformer-circuits & depth structure
%=============================================================================

@article{elhage2021mathematical,
    title={A Mathematical Framework for Transformer Circuits},
    author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and others},
    journal={Transformer Circuits Thread},
    year={2021},
    url={https://transformer-circuits.pub/2021/framework/index.html}
}

@article{olsson2022context,
    title={In-context Learning and Induction Heads},
    author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and others},
    journal={arXiv preprint arXiv:2209.11895},
    year={2022},
    url={https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html}
}

@article{rai2024practical,
    title={A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models},
    author={Rai, Daking and Lee, Yilun and Xuan, Shi Feng and Yao, Leif and Kwan, Jeffrey and Mitchell, Eric and Finn, Chelsea},
    journal={arXiv preprint arXiv:2407.02646},
    year={2024}
}

% Induction, pattern heads, and in-context learning
%=============================================================================

@article{wang2025which,
    title={Which Attention Heads Matter for In-Context Learning?},
    author={Wang, Lidong and others},
    journal={arXiv preprint},
    year={2025}
}

@inproceedings{zhang2025induction,
    title={Induction Heads as an Essential Mechanism for Pattern Matching in In-Context Learning},
    author={Zhang, Jingze and others},
    booktitle={Findings of the Association for Computational Linguistics: NAACL 2025},
    year={2025}
}

% IOI / Name-mover / Copy-suppression / S-inhibition heads
%=============================================================================

@article{bills2023language,
    title={Language models can explain neurons in language models},
    author={Bills, Steven and Cammarata, Nick and Mossing, Dan and Tillman, Henk and Gao, Leo and Goh, Gabriel and Sutskever, Ilya and Leike, Jan and Wu, Jeff and Saunders, William},
    journal={OpenAI Blog},
    year={2023},
    url={https://openai.com/research/language-models-can-explain-neurons-in-language-models}
}

@article{wang2022interpretability,
    title={Interpretability in the Wild: A Circuit for Indirect Object Identification in GPT-2 Small},
    author={Wang, Kevin and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
    journal={arXiv preprint arXiv:2211.00593},
    year={2022}
}

% Safety, refusal, policy, and "safety stack"
%=============================================================================

@article{zhou2025refusal,
    title={Refusal Falls off a Cliff: How Safety Alignment Fails in Reasoning?},
    author={Zhou, Andy and others},
    journal={OpenReview},
    year={2025},
    url={https://openreview.net}
}

@article{arditi2024refusal,
    title={Refusal in LLMs is Mediated by a Single Direction},
    author={Arditi, Andy and Obeso, Oscar and Kreutzer, Aaquib and Rager, Alex and Jenner, Eric and Prakash, Esben and Belrose, Nora and Turner, Alex},
    journal={arXiv preprint},
    year={2024}
}

% Additional interpretability work
%=============================================================================

@article{zheng2025attention,
    title={Attention Heads of Large Language Models: A Survey},
    author={Zheng, Zifan and Wang, Yezhaohui and Huang, Yuxin and Song, Shichao and Yang, Mingchuan and Tang, Bo and Xiong, Feiyu and Li, Zhiyu},
    journal={Patterns},
    volume={6},
    number={2},
    pages={101176},
    year={2025},
    publisher={Elsevier},
    doi={10.1016/j.patter.2025.101176}
}

@article{voita2019analyzing,
    title={Analyzing Multi-Head Self-Attention: Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned},
    author={Voita, Elena and Talbot, David and Moiseev, Fedor and Sennrich, Rico and Titov, Ivan},
    journal={arXiv preprint arXiv:1905.09418},
    year={2019}
}

@inproceedings{michel2019sixteen,
    title={Are Sixteen Heads Really Better than One?},
    author={Michel, Paul and Levy, Omer and Neubig, Graham},
    booktitle={Advances in Neural Information Processing Systems},
    volume={32},
    year={2019}
}

% Transformer architecture
%=============================================================================

@article{vaswani2017attention,
    title={Attention is All You Need},
    author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
    journal={Advances in Neural Information Processing Systems},
    volume={30},
    year={2017}
}

% Additional mechanistic interpretability resources
%=============================================================================

@misc{nanda2022walkthrough,
    title={A Walkthrough of Interpretability in the Wild (IOI)},
    author={Nanda, Neel},
    year={2022},
    url={https://www.neelnanda.io}
}

@misc{transformer_circuits,
    title={Transformer Circuits Thread},
    author={{Anthropic}},
    year={2021--2024},
    url={https://transformer-circuits.pub}
}

% Chain-of-thought and reasoning
%=============================================================================

@article{wei2022chain,
    title={Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
    author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
    journal={Advances in Neural Information Processing Systems},
    volume={35},
    pages={24824--24837},
    year={2022}
}

% Code generation and programming
%=============================================================================

@article{chen2021evaluating,
    title={Evaluating Large Language Models Trained on Code},
    author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
    journal={arXiv preprint arXiv:2107.03374},
    year={2021}
}

% Mathematical reasoning
%=============================================================================

@article{lewkowycz2022solving,
    title={Solving Quantitative Reasoning Problems with Language Models},
    author={Lewkowycz, Aitor and Andreassen, Anders and Dohan, David and Dyer, Ethan and Michalewski, Henryk and Ramasesh, Vinay and Slone, Ambrose and Anil, Cem and Schlag, Imanol and Gutman-Solo, Theo and others},
    journal={Advances in Neural Information Processing Systems},
    volume={35},
    pages={3843--3857},
    year={2022}
}

% Safety and alignment
%=============================================================================

@article{ouyang2022training,
    title={Training Language Models to Follow Instructions with Human Feedback},
    author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
    journal={Advances in Neural Information Processing Systems},
    volume={35},
    pages={27730--27744},
    year={2022}
}

@article{bai2022constitutional,
    title={Constitutional AI: Harmlessness from AI Feedback},
    author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
    journal={arXiv preprint arXiv:2212.08073},
    year={2022}
}

% LLM surveys and reviews
%=============================================================================

@article{brown2020language,
    title={Language Models are Few-Shot Learners},
    author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
    journal={Advances in Neural Information Processing Systems},
    volume={33},
    pages={1877--1901},
    year={2020}
}

@article{touvron2023llama,
    title={LLaMA: Open and Efficient Foundation Language Models},
    author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
    journal={arXiv preprint arXiv:2302.13971},
    year={2023}
}

@article{achiam2023gpt,
    title={GPT-4 Technical Report},
    author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
    journal={arXiv preprint arXiv:2303.08774},
    year={2023}
}

%=============================================================================
% End of Bibliography
%=============================================================================
