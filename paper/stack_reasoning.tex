%=============================================================================
\subsection{Reasoning \& Algorithmic Stack}
\label{sec:reasoning-stack} \textbf{Stack overview:} This stack encompasses heads that perform pattern matching, sequence continuation, algorithmic reasoning, strategic planning, and meta-cognitive oversight. These heads enable in-context learning, pattern completion, systematic token prediction, and higher-level reasoning quality control. %-----------------------------------------------------------------------------
\subsubsection{(E) Previous-Token Heads}
\label{head:previous-token} \noindent\depthinfo{0.05--0.18} | \litnames{previous-token head, shift head, offset head} \begin{functiondesc}
Copies information from each token to the position of the next token, creating a shifted representation where token $t$ contains information about token $t-1$. This appears to be a foundational component of induction circuits, enabling later heads to access "what came before" without directly attending backwards. Helps implement a simple but crucial transformation that allows pattern matching across the sequence. The head typically shows a strong diagonal attention pattern (attending from position $i$ to position $i-1$).
\end{functiondesc} \begin{attentionbox}
\attstrong{Immediately preceding token (diagonal attention pattern)}\\
\attweak{Distant tokens, same-position tokens}\\
\attreacts{Sequential structure, token boundaries}
\end{attentionbox} \begin{ablationbox}
\textbf{Expected ablation:} Breaks induction circuits entirely, causing degradation in pattern completion tasks. Induction heads become unable to access "what came after previous occurrences" since that information is no longer shifted forward. Important for in-context learning.
\end{ablationbox} \begin{examplebox}
\exinput{"The cat sat. The cat..."}\\
\exbehavior{Copies "The" to position after "The", "cat" to position after "cat", etc.}\\
\exeffect{Later induction heads can match "cat" and access what followed it ("sat")}
\end{examplebox} \headfooter{\statuswell}{induction head (M), duplicate-token (M)} %-----------------------------------------------------------------------------
\subsubsection{(E) Local Pattern Heads}
\label{head:local-pattern} \noindent\depthinfo{0.08--0.20} | \litnames{local pattern head, char-level head, n-gram head} \begin{functiondesc}
Detects and processes local character-level or subword patterns, particularly useful for handling spelling, capitalization, punctuation patterns, and morphological structure. Operates at a finer granularity than most heads, attending to patterns within and between adjacent tokens. Important for tasks like spell checking, case handling, and recognizing common subword patterns. May also detect repeated character sequences or structural patterns like "ing", "tion", or punctuation clusters.
\end{functiondesc} \begin{attentionbox}
\attstrong{Adjacent tokens, subword units, character-level patterns}\\
\attweak{Long-range dependencies, semantic content}\\
\attreacts{Spelling patterns, capitalization, punctuation, morphology}
\end{attentionbox} \begin{ablationbox}
\textbf{Expected ablation:} Degradation in handling of misspellings, case variations, and morphological patterns. increase in errors on tasks requiring character-level awareness. Partial fallback through tokenization and other pattern heads.
\end{ablationbox} \begin{examplebox}
\exinput{"The organizATION's" (unusual capitalization)}\\
\exbehavior{Detects unusual case pattern in "ATION", attends to surrounding context}\\
\exeffect{Helps model handle non-standard capitalization correctly}
\end{examplebox} \headfooter{\statusobs}{induction head (M), duplicate-token (M)} %-----------------------------------------------------------------------------
\subsubsection{(M) Induction Heads}
\label{head:induction} \noindent\depthinfo{0.30--0.65} | \litnames{induction head, pattern head, copy head, ICL head} \begin{functiondesc}
Detects repeated subsequences of the form [A][B]...[A] and predicts that [B] should follow the second [A]. Operates by attending to tokens that appeared after previous instances of the current token. Works in conjunction with previous-token heads which copy information about what preceded each token. This mechanism appears fundamental to in-context learning, enabling pattern completion, name recall, and few-shot learning without parameter updates. One of the most well-documented and important head types in transformer interpretability.
\end{functiondesc} \begin{attentionbox}
\attstrong{Tokens following previous occurrences of current token}\\
\attweak{Immediate neighbors, first occurrence, unrelated tokens}\\
\attreacts{Token repetition, [A][B]...[A] patterns, contextual recurrence}
\end{attentionbox} \begin{ablationbox}
\textbf{Expected ablation:} Significant degradation in in-context learning tasks, reduced pattern completion and few-shot learning. Model may partially compensate through other heads but with substantial accuracy loss. Important for ICL capability.
\end{ablationbox} \begin{examplebox}
\exinput{"When Mary and John went to the store, Mary bought..."}\\
\exbehavior{Second "Mary" attends to tokens following first "Mary" (especially "and")}\\
\exeffect{Increased probability of contextually appropriate continuation}
\end{examplebox} \headfooter{\statuswell}{previous-token (E), duplicate-token (M), name-mover (L)} %-----------------------------------------------------------------------------
\subsubsection{(M) Duplicate-Token Heads}
\label{head:duplicate-token} \noindent\depthinfo{0.35--0.60} | \litnames{duplicate-token head, repetition head, copy head} \begin{functiondesc}
Detects when the current token has appeared previously in the sequence, marking repeated tokens for downstream processing. Unlike induction heads which predict what comes next, duplicate-token heads simply signal "this token appeared before". This information is used by various circuits including IOI (indirect object identification), name-mover heads, and copy-suppression mechanisms. Helps implement a simpler form of pattern matching than full induction, serving as a building block for more complex behaviors.
\end{functiondesc} \begin{attentionbox}
\attstrong{Previous identical tokens (exact matches)}\\
\attweak{Similar but non-identical tokens, first occurrence}\\
\attreacts{Exact token repetition, name recurrence, repeated phrases}
\end{attentionbox} \begin{ablationbox}
\textbf{Expected ablation:} Impaired duplicate detection, affecting name-mover circuits and copy-suppression. degradation in tasks requiring duplicate awareness. Partial overlap with induction heads provides some redundancy.
\end{ablationbox} \begin{examplebox}
\exinput{"Alice gave the book to Bob. Then Alice..."}\\
\exbehavior{Second "Alice" detects it appeared earlier, writes duplicate signal}\\
\exeffect{Downstream heads (name-movers, S-inhibition) use this signal}
\end{examplebox} \headfooter{\statuswell}{induction (M), name-mover (L), S-inhibition (L)} %-----------------------------------------------------------------------------
\subsubsection{(M) Skip-Trigram Heads}
\label{head:skip-trigram} \noindent\depthinfo{0.40--0.65} | \litnames{skip-trigram head, skip-gram head} \begin{functiondesc}
Implements skip-gram pattern matching, attending to non-contiguous patterns like [A]...[B]...[C] where the dots represent intervening tokens. More flexible than strict n-gram matching, allowing for pattern recognition across variable distances. Useful for detecting phrasal patterns, idiomatic expressions, and structural templates with flexible word order. Generalizes beyond strict adjacency requirements while maintaining pattern specificity.
\end{functiondesc} \begin{attentionbox}
\attstrong{Pattern components separated by 1-3 tokens}\\
\attweak{Strictly adjacent patterns, very long-range dependencies}\\
\attreacts{Phrasal patterns, templates, flexible idioms}
\end{attentionbox} \begin{ablationbox}
\textbf{Expected ablation:} Reduced recognition of flexible patterns and templates. degradation on tasks requiring non-contiguous pattern matching. Less critical than induction heads; other pattern mechanisms provide fallback.
\end{ablationbox} \begin{examplebox}
\exinput{"not only X but also" (skip-bigram pattern)}\\
\exbehavior{Recognizes "not...but" pattern despite intervening tokens}\\
\exeffect{Helps predict "also" after "but" even with intervening content}
\end{examplebox} \headfooter{\statusobs}{induction (M), local-pattern (E)} %-----------------------------------------------------------------------------
\subsubsection{(M) Algorithmic Continuation Heads}
\label{head:algorithmic-continuation} \noindent\depthinfo{0.45--0.70} | \litnames{algorithmic head, continuation head, sequence head} \begin{functiondesc}
Recognizes and continues algorithmic sequences such as counting (1, 2, 3...), days of week, months, or other systematic progressions. Distinct from general pattern matching by operating on sequences with clear algorithmic rules. Can detect arithmetic progressions, cyclic patterns, and other rule-governed sequences. Contributes to the model's ability to perform basic reasoning over structured sequences without explicit training on those specific patterns.
\end{functiondesc} \begin{attentionbox}
\attstrong{Sequential elements in algorithmic patterns (numbers, ordered lists)}\\
\attweak{Random sequences, semantic patterns without algorithmic structure}\\
\attreacts{Arithmetic progressions, cyclic orderings, systematic enumerations}
\end{attentionbox} \begin{ablationbox}
\textbf{Expected ablation:} Reduced performance on sequence continuation tasks (counting, ordering). degradation on arithmetic sequences and structured enumerations. Some algorithmic reasoning may persist through other mechanisms.
\end{ablationbox} \begin{examplebox}
\exinput{"Monday, Tuesday, Wednesday, ..."}\\
\exbehavior{Recognizes day-of-week sequence, attends to progression pattern}\\
\exeffect{Strongly predicts "Thursday" as next token}
\end{examplebox} \headfooter{\statusobs}{induction (M), digit (M)} %-----------------------------------------------------------------------------
\subsubsection{(L) Strategy Heads}
\label{head:strategy} \noindent\depthinfo{0.68--0.88} | \litnames{strategy head, planning head, strategy-switching head, approach-selection head, approach-adaptation head, pivot head} \begin{functiondesc}
Plans overall approach and strategy for complex tasks, and adapts strategy when current approach proves ineffective. Influences high-level structure such as whether to break into steps, what order to address components, and which methods to apply. Operates before detailed execution to establish the strategic framework. Recognizes different task types that may require different approaches, such as analytical versus creative reasoning, or sequential versus parallel processing. Can decompose complex queries into manageable subtasks. Also detects when the current reasoning strategy is not working effectively and switches to alternative approaches. Monitors problem-solving progress and recognizes dead ends, insufficient methods, or the need for different techniques. May pivot from analytical to creative approaches, from depth-first to breadth-first exploration, or from deductive to inductive reasoning. Useful for robust problem-solving across varied challenges and effective handling of multi-part or complex queries.
\end{functiondesc} \begin{attentionbox}
\attstrong{Task complexity indicators, multi-part queries, strategic choice points, progress indicators, approach effectiveness signals}\\
\attweak{Simple single-step tasks, purely reactive responses, successfully progressing solutions}\\
\attreacts{Complex tasks, planning requests, multi-step problems, strategic decision points, signs of insufficient progress}
\end{attentionbox} \begin{ablationbox}
\textbf{Expected ablation:} Reduced strategic planning quality and adaptability. May jump into execution without appropriate planning or continue unproductive approaches longer than optimal. Complex tasks handled less efficiently with reduced ability to recover from initially incorrect approaches.
\end{ablationbox} \begin{examplebox}
\exinput{"Help me plan a machine learning project to predict customer churn."}\\
\exbehavior{Recognizes need for structured planning, breaks into phases}\\
\exeffect{Response structures approach: data collection $\rightarrow$ exploratory analysis $\rightarrow$ feature engineering $\rightarrow$ model selection $\rightarrow$ evaluation}
\end{examplebox} \headfooter{\statusobs}{reasoning-oversight (F)} %-----------------------------------------------------------------------------
\subsubsection{(F) Reasoning-Oversight Heads}
\label{head:reasoning-oversight}

\noindent\depthinfo{0.88--0.99} | \litnames{reasoning-mode head, thinking-style head, cognitive-mode head, reasoning head, meta-reasoning head, meta-CoT head, reasoning-reflection head, thought-monitoring head, cognitive-oversight head, reasoning-quality head}

\begin{functiondesc}
Provides highest-level management of reasoning processes, including mode selection and quality monitoring. These heads select and maintain appropriate reasoning modes for different tasks (analytical mode for precise logical problems, creative mode for brainstorming, analogical mode for novel domains, deductive versus inductive reasoning for different inference types) while simultaneously monitoring the quality and direction of reasoning chains. They identify when more thought is needed, detect reasoning errors or gaps, flag uncertain conclusions, and can trigger re-thinking or additional reasoning steps. Operating at a meta-level above regular chain-of-thought reasoning, these heads help ensure reasoning chains are sound, complete, and appropriately matched to task requirements. They influence which reasoning patterns and heuristics become active and act as cognitive oversight to help prevent confident errors in complex reasoning scenarios.
\end{functiondesc}

\begin{attentionbox}
\attstrong{Task type indicators, reasoning mode cues, cognitive approach requirements, reasoning quality indicators, logical gaps, uncertainty signals, reasoning chain progress, consistency checks}\\
\attweak{Mode-independent content, simple factual responses, non-reasoning tasks, straightforward answers}\\
\attreacts{Problem types, explicit mode requests, task characteristics suggesting optimal approach, complex reasoning tasks, logical steps, argument quality, reasoning errors, inconsistencies}
\end{attentionbox}

\begin{ablationbox}
\textbf{Expected ablation:} Less appropriate reasoning mode selection and reduced reasoning quality oversight. May use analytical mode for creative tasks or vice versa, reducing effectiveness. More logical gaps, less self-correction, reduced reasoning completeness. Chain-of-thought reasoning becomes less reliable, particularly for complex or multi-step problems. Reasoning remains functional but less well-suited to specific task requirements.
\end{ablationbox}

\begin{examplebox}
\exinput{"Brainstorm creative names" or [Complex logical problem requiring multi-step reasoning]}\\
\exbehavior{Selects creative/generative mode rather than analytical; monitors reasoning chain, detects gap: "Wait, I should verify this assumption..."}\\
\exeffect{Free-flowing creative suggestions + improved reasoning quality through self-monitoring}
\end{examplebox}

\headfooter{\statusobs}{strategy (L), step-by-step (F)}
