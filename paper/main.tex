\documentclass[11pt,a4paper]{article}

% Load preamble with packages and macros
\input{preamble}

% Document metadata
\title{Attention Head Naming Convention \\ for Large Language Models (LLMs)}
\author{Karol Kowalczyk}
\date{November 2025}

\begin{document}

\maketitle

\begin{abstract}
Large language models exhibit remarkable reasoning, safety alignment, and structural understanding, yet their internal workings remain opaque. Attention heads---specialized components within transformer layers---have emerged as key objects of study in interpretability research. The community has developed informal names: \emph{induction heads}, \emph{name mover heads}, \emph{refusal heads}, but these terms are inconsistent, overlapping, and ambiguous.

This work proposes a unified naming convention for attention heads: (1) a four-level depth model (Early, Middle, Late, Final), (2) stack-based functional grouping, (3) canonical names for head types, and (4) cross-reference tables mapping historical terms to standardized ones. This taxonomy is descriptive rather than prescriptive, capturing current head behaviors while remaining flexible for future architectures.
\end{abstract}

\tableofcontents
\clearpage

%=============================================================================
% MAIN CONTENT
%=============================================================================

\section{Introduction}
\label{sec:introduction}

\subsection{Motivation}
Attention heads---the basic computational units within transformer architectures---have emerged as key objects of study in mechanistic interpretability research despite achieving remarkable performance across diverse tasks.

\subsection{The Problem of Inconsistent Naming}
The interpretability community has identified numerous specialized attention head types: \emph{induction heads}, \emph{name mover heads}, \emph{refusal heads}, \emph{delimiter heads}, and \emph{JSON heads}. These naming conventions are \textbf{inconsistent} (same head type, multiple names), \textbf{ambiguous} (single name, different behaviors), \textbf{fragmented} (no unified framework), and \textbf{unscalable} (fail across architectures). This fragmentation complicates replication, comparison, and dataset annotation.

\subsection{Goals of This Work}
I propose a unified naming convention that standardizes terminology, provides a functional taxonomy grounded in empirical observations, describes head behavior consistently across architectures, and creates stable vocabulary that evolves with models.

\subsection{Circuits, Stacks, and Simplification}

This taxonomy uses \emph{stacks} as organizational framework. Attention heads work in complex \emph{circuits}---groups across layers cooperating through multi-level processing~\cite{elhage2021mathematical,wang2022interpretability}. 

The \emph{stack} abstraction simplifies this complexity for communication. Rather than mapping every circuit connection, I group heads by primary functional contribution, making the taxonomy accessible while acknowledging that real model behavior involves intricate cross-layer interactions.

\subsection{Structure of This Document}
I review prior work (\S\ref{sec:background}), introduce the depth model (\S\ref{sec:depth}) and stacks (\S\ref{sec:stacks}), present a comprehensive catalog organized by functional stack (\S\ref{sec:catalog}), and conclude with discussion (\S\ref{sec:discussion}) and future directions (\S\ref{sec:conclusion}).

%=============================================================================
\section{Background}
\label{sec:background}

\subsection{Attention Heads and Functions}
In transformer models~\cite{vaswani2017attention}, attention heads perform focused computations over token sequences. Though individually simple, they develop specialized behaviors: pattern continuation, entity tracking, semantic filtering, routing, format enforcement, and safety constraints~\cite{elhage2021mathematical,olsson2022context}. These behaviors form \emph{circuits} and larger \emph{stacks} of related functionality.

\subsection{Why Naming Consistency Matters}
Interpretability research suffers from fragmented terminology~\cite{rai2024practical,zheng2025attention}. The same head type appears under multiple names, while overloaded names refer to unrelated behaviors. Consistent naming improves communication clarity, strengthens cross-paper alignment, helps index interpretability datasets, and enables systematic circuit mapping.

\subsection{Prior Naming Practices}
Previous work named heads by behavior (induction, copy-suppression), formatting (JSON, list), signal source (delimiter), circuit role (name mover), or safety function (refusal, toxicity). Though often accurate, these labels vary widely. This work unifies them under a systematic framework.

%=============================================================================
\section{Depth Model: Early---Middle---Late---Final}
\label{sec:depth}

\subsection{Rationale for Four Depth Categories}
Functional behavior clusters reliably into four zones~\cite{elhage2021mathematical,wang2022interpretability}: \textbf{Early (E)} layers handle token-level processing, boundary detection, and filtering. \textbf{Middle (M)} layers implement reasoning primitives, induction, and dependency tracking. \textbf{Late (L)} layers perform semantic integration, routing, and persona shaping. \textbf{Final (F)} layers enforce policy, safety, and structured output. This structure holds across GPT, LLaMA, and Claude~\cite{brown2020language,touvron2023llama,achiam2023gpt}.

\subsection{Cross-Model Depth Examples}
Using \emph{relative depth} (0.0--1.0) makes the taxonomy scale-free. For a 96-layer model: Early = layers 0--15 (0.00--0.15), Middle = 15--50 (0.15--0.52), Late = 50--85 (0.52--0.88), Final = 85--96 (0.88--1.00).

\subsection{Relative Depth Scaling}
I express depth as fraction of total model depth for cross-architecture comparison. A head at relative depth 0.40 occupies similar functional space in 12-layer or 96-layer models.

%=============================================================================
\section{Stacks: Functional Grouping of Attention Heads}
\label{sec:stacks}

\subsection{What is a Stack?}
A \emph{stack} groups head types that together implement a higher-level capability. Stacks reflect functional clustering observed in studies~\cite{wang2022interpretability,olsson2022context}. Examples: Reasoning \& Algorithmic, Memory \& Dependency, Safety, Output Formatting. Stacks span Early, Middle, Late, and Final layers.

\subsection{Relationship Between Stacks and Depth}
Different functions appear at different depths. Early: delimiters, content detection, input conditioning. Middle: reasoning, induction, entity linking. Late: narrative coherence, routing, topic steering. Final: policy, formatting, rewriting, safety. This \emph{stack $\times$ depth} structure forms the catalog basis.

%=============================================================================
% CATALOG
%=============================================================================

\section{Attention Head Catalog}
\label{sec:catalog}

This section catalogs attention head types by functional stack. Each stack groups heads contributing to common high-level capability, ordered by depth (Early $\rightarrow$ Middle $\rightarrow$ Late $\rightarrow$ Final).

\paragraph{Entry Format.} Each head entry includes:
\begin{itemize}
    \item \textbf{Depth range:} Typical relative depth (0.0--1.0)
    \item \textbf{Literature names:} Alternative names from prior work
    \item \textbf{Function:} Core behavior and mechanism
    \item \textbf{Attention pattern:} What the heads attend to
    \item \textbf{Expected ablation:} Predicted effects if disabled
    \item \textbf{Example scenario:} Concrete behavioral illustration
    \item \textbf{Stack and relations:} Primary stack and related heads
\end{itemize}

% Individual stack files
\input{stack_reasoning}
\input{stack_memory}
\input{stack_instruction}
\input{stack_knowledge}
\input{stack_safety}
\input{stack_routing}
\input{stack_structural}
\input{stack_formatting}
\input{stack_stylistic}

%=============================================================================
\section{Discussion}
\label{sec:discussion}

\subsection{Cross-Stack Patterns}
Consistent patterns emerge across architectures~\cite{rai2024practical,zheng2025attention}. Early heads operate on surface features. Middle heads contain the computational core. Late heads integrate high-level semantics. Final heads handle policy, safety, and structural correctness. Stacks combine heads from multiple depths.

\subsection{Depth Distribution Across Stacks}
Stacks concentrate at specific depths. Structural \& Boundary and Safety (detection) are Early-heavy. Reasoning \& Algorithmic and Memory \& Dependency are Middle-heavy. Knowledge Retrieval and Stylistic \& Persona are Late-heavy. Safety (enforcement) and Output Formatting are Final-heavy. This reflects hierarchical processing flow.

\subsection{Ambiguous or Multi-Role Heads}
Some heads perform multiple functions depending on context, circuit interactions, or model architecture~\cite{voita2019analyzing}. I name heads by \textbf{primary, reproducible function}, noting secondary behaviors in descriptions.

\subsection{Model-Specific Variations}
Most head types appear consistently across architectures. GPT-style models emphasize certain reasoning heads~\cite{brown2020language}, LLaMA models show strong instruction-following patterns~\cite{touvron2023llama}, and safety-tuned models have pronounced safety stack heads~\cite{ouyang2022training,bai2022constitutional}. This taxonomy accommodates variations through depth ranges and status indicators.

\subsection{Limitations and Future Work}
This naming convention has limitations:

\paragraph{Scope.} Focus on attention heads; MLPs, embeddings, and other components also contribute.

\paragraph{Empirical Grounding.} Many entries synthesize literature reports rather than presenting novel findings. Future work should validate these categorizations.

\paragraph{Architecture Evolution.} New architectures (e.g., different attention mechanisms) may require extensions.

\paragraph{Head Polysemanticity.} Some heads serve multiple functions that single names cannot capture.

Despite limitations, this taxonomy provides valuable organizing framework.

%=============================================================================
\section{Conclusion}
\label{sec:conclusion}

\subsection{Summary of Contributions}
This work introduces a unified naming framework for attention heads in transformer models: four-level depth model (Early/Middle/Late/Final), stack-based functional taxonomy (nine stacks), canonical names, and comprehensive cross-reference for historical terminology.

\subsection{Adoption Guidelines}
I recommend researchers use canonical names in papers, include alternatives in parentheses when first mentioned, specify depth ranges when reporting discoveries, and indicate primary stack membership. Example: ``I identified an induction head (pattern head) at relative depth 0.35 in the Reasoning \& Algorithmic stack.''

\subsection{Future Directions}
This taxonomy opens research directions:

\paragraph{Empirical Validation.} Systematic studies validating head types across models~\cite{rai2024practical,zheng2025attention}.

\paragraph{Automated Detection.} Tools for automatically identifying and classifying heads~\cite{bills2023language}.

\paragraph{Circuit Mapping.} Using standardized names to build comprehensive circuit databases~\cite{wang2022interpretability}.

\paragraph{Architecture Design.} Leveraging taxonomy to design more interpretable models.

\paragraph{Safety Applications.} Using head understanding to improve alignment and safety~\cite{zhou2025refusal,arditi2024refusal}.

This naming convention facilitates communication, enables replication, and provides structure to the expanding field.

%=============================================================================
% APPENDICES
%=============================================================================
\clearpage
\appendix

\input{appendix}

%=============================================================================
% BIBLIOGRAPHY
%=============================================================================
\clearpage
\bibliographystyle{plainnat}
\bibliography{bibliography}

\end{document}
