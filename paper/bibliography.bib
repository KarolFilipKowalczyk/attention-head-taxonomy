%=============================================================================
% BIBLIOGRAPHY.BIB - References for Attention Head Naming Convention
%=============================================================================

@article{elhage2021mathematical,
    title={A Mathematical Framework for Transformer Circuits},
    author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and others},
    journal={Transformer Circuits Thread},
    year={2021},
    url={https://transformer-circuits.pub/2021/framework/index.html}
}

@article{olsson2022context,
    title={In-context Learning and Induction Heads},
    author={Olsson, Catherine and Elhage, Nelson and Nanda, Neel and others},
    journal={arXiv preprint arXiv:2209.11895},
    year={2022}
}

@article{vaswani2017attention,
    title={Attention is All You Need},
    author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and others},
    journal={Advances in Neural Information Processing Systems},
    volume={30},
    year={2017}
}

@article{wang2022interpretability,
    title={Interpretability in the Wild: A Circuit for Indirect Object Identification in GPT-2 Small},
    author={Wang, Kevin and Variengien, Alexandre and Conmy, Arthur and others},
    journal={arXiv preprint arXiv:2211.00593},
    year={2022}
}

@article{rai2024practical,
    title={A Practical Review of Mechanistic Interpretability for Transformer-Based Language Models},
    author={Rai, Daking and Lee, Yilun and others},
    journal={arXiv preprint arXiv:2407.02646},
    year={2024}
}

@article{zheng2025attention,
    title={Attention Heads of Large Language Models: A Survey},
    author={Zheng, Zifan and Wang, Yezhaohui and others},
    journal={Patterns},
    year={2025}
}

@article{voita2019analyzing,
    title={Analyzing Multi-Head Self-Attention},
    author={Voita, Elena and Talbot, David and others},
    journal={arXiv preprint arXiv:1905.09418},
    year={2019}
}

@article{brown2020language,
    title={Language Models are Few-Shot Learners},
    author={Brown, Tom and Mann, Benjamin and others},
    journal={Advances in Neural Information Processing Systems},
    year={2020}
}

@article{touvron2023llama,
    title={LLaMA: Open and Efficient Foundation Language Models},
    author={Touvron, Hugo and Lavril, Thibaut and others},
    year={2023}
}

@article{achiam2023gpt,
    title={GPT-4 Technical Report},
    author={Achiam, Josh and Adler, Steven and others},
    year={2023}
}

@article{ouyang2022training,
    title={Training Language Models to Follow Instructions with Human Feedback},
    author={Ouyang, Long and Wu, Jeffrey and others},
    year={2022}
}

@article{bai2022constitutional,
    title={Constitutional AI: Harmlessness from AI Feedback},
    author={Bai, Yuntao and Kadavath, Saurav and others},
    year={2022}
}

@article{bills2023language,
    title={Language models can explain neurons in language models},
    author={Bills, Steven and Cammarata, Nick and others},
    year={2023}
}

@article{zhou2025refusal,
    title={Refusal Falls off a Cliff: How Safety Alignment Fails in Reasoning?},
    author={Zhou, Andy and others},
    year={2025}
}

@article{arditi2024refusal,
    title={Refusal in LLMs is Mediated by a Single Direction},
    author={Arditi, Andy and Obeso, Oscar and others},
    year={2024}
}
